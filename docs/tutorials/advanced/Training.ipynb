{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we provide advanced examples to alter a number of objects in the end2you pipeline. In particular, we provide the following examples:\n",
    "\n",
    "* [Data Provder: Process input](): Pre-process the raw input signal before feeding it to the network.\n",
    "* [Custom Audio Model](): Use your own network architecture\n",
    "* [Custom Loss Function](): Use your own loss function for training a model\n",
    "* [Custom Evaluation Function](): Use your own metric function to evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Provder: Process input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the ability to pre-process the input data by defining a custom data provider. The custom provider needs to inherit from a data provider of end2you, and the pre-processing is applied in the `process_input` method.\n",
    "\n",
    "For example, if we want to apply a pre-emphasis filter on the raw audio signal we can define the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from end2you.data_provider import AudioProvider\n",
    "from end2you.data_provider.get_provider import pad_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CustomAudioProvider(AudioProvider):\n",
    "    \"\"\"Provides the data for the audio modality.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.modality = 'audio'\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def process_input(self, data, labels):\n",
    "        \"\"\" Pre-process input.\n",
    "        \n",
    "        Args:\n",
    "          data (np.array) (S x T): Raw audio signal with S frames and T number of samples.\n",
    "          labels (np.array) (S x N) : Data labels with N outputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        processed_data = []\n",
    "        for i, di in enumerate(data):\n",
    "            # Apply pre-emphasis filter \n",
    "            processed_data.append(\n",
    "                np.hstack([di[0], di[1:] - 0.97 * di[:-1]])\n",
    "            )\n",
    "        \n",
    "        processed_data = np.vstack(processed_data)\n",
    "        \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke the `CustomAudioProvider` class and then the `DataLoader` class of pytorch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(CustomProvider, params, **kwargs):\n",
    "    \n",
    "    provider_class = CustomProvider(params.dataset_path, seq_length=params.seq_length)\n",
    "    \n",
    "    return DataLoader(provider_class,\n",
    "                      batch_size=params.batch_size,\n",
    "                      shuffle=params.is_training,\n",
    "                      num_workers=params.num_workers,\n",
    "                      pin_memory=params.cuda,\n",
    "                      collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of parameters can be defined to get instantiate the `DataLoader` class of pytorch. As we have a training and a validation processes, we need to define two loaders; one for training and the other for evaluation. \n",
    "\n",
    "We define the `_get_params` method to return the required loaders as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from end2you.utils import Params\n",
    "\n",
    "\n",
    "def _get_params(modality:str,\n",
    "               dataset_paths:list,\n",
    "               seq_length:int,\n",
    "               batch_size:int,\n",
    "               cuda:bool):\n",
    "    train_params = Params(dict_params={\n",
    "        'modality': modality,\n",
    "        'dataset_path': dataset_paths[0],\n",
    "        'seq_length': seq_length,\n",
    "        'batch_size': batch_size,\n",
    "        'cuda': cuda,\n",
    "        'num_workers': 0,\n",
    "        'is_training': True,\n",
    "\n",
    "    })\n",
    "    \n",
    "    valid_params = Params(dict_params={\n",
    "        'modality': modality,\n",
    "        'dataset_path': dataset_paths[1],\n",
    "        'seq_length': seq_length,\n",
    "        'batch_size': batch_size,\n",
    "        'cuda': cuda,\n",
    "        'num_workers': 0,\n",
    "        'is_training': False,\n",
    "\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'train': get_dataloader(CustomAudioProvider, train_params),\n",
    "        'valid': get_dataloader(CustomAudioProvider, valid_params)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you need to define the path to the training and validation `hdf5` files, which were created using the data generator (see [basic](../basic))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_providers = _get_params(\n",
    "    modality='audio',\n",
    "    dataset_paths=['/path/to/train/hdf5/files', '/path/to/valid/hdf5/files'],\n",
    "    seq_length=150,\n",
    "    batch_size=10,\n",
    "    cuda=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Audio Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the flexibility to use your own network architecture. To do so, you just need to define your model under the PyTorch framework and feed it to the training process. \n",
    "\n",
    "The examples that follows builds an audio network. An important property of the class is the `num_outs` parameter, which is the number of predictions of the model. In this notebook we use the SEWA database where 3 outputs are provided, and hence  in this case `num_outs = 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from end2you.models.audio import AudioModel\n",
    "from end2you.models.rnn import RNN\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size:int,\n",
    "                 num_outs:int,\n",
    "                 pretrained:bool = False,\n",
    "                 model_name:str = None):\n",
    "        \"\"\" Convolutional recurrent neural network model.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Input size to the model. \n",
    "            num_outs (int): Number of output values of the model.\n",
    "            pretrained (bool): Use pretrain model (default `False`).\n",
    "            model_name (str): Name of model to build (default `None`).\n",
    "        \"\"\"\n",
    "        \n",
    "        super(AudioRNNModel, self).__init__()\n",
    "        audio_network = AudioModel(model_name=model_name, input_size=input_size)\n",
    "        self.audio_model = audio_network.model\n",
    "        num_out_features = audio_network.num_features\n",
    "        self.rnn, num_out_features = self._get_rnn_model(num_out_features)\n",
    "        self.linear = nn.Linear(num_out_features, num_outs)\n",
    "        self.num_outs = num_outs\n",
    "    \n",
    "    def _get_rnn_model(self, input_size:int):\n",
    "        \"\"\" Builder method to get RNN instace.\"\"\"\n",
    "        \n",
    "        rnn_args = {\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 2,\n",
    "            'batch_first':True\n",
    "        }\n",
    "        return RNN(rnn_args, 'gru'), rnn_args['hidden_size']\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x ((torch.Tensor) - BS x S x 1 x T)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_length, t = x.shape\n",
    "        x = x.view(batch_size*seq_length, 1, t)\n",
    "        \n",
    "        audio_out = self.audio_model(x)\n",
    "        audio_out = audio_out.view(batch_size, seq_length, -1)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(audio_out)\n",
    "        \n",
    "        output = self.linear(rnn_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioRNNModel(\n",
    "    input_size=1600, num_outs=3, model_name='emo18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the ability to define your own loss function and use it for training your model. To do so, you need to define a class that inherits from end2you's `Losses` class and define your metric as method in the class. \n",
    "\n",
    "An example to define MSE loss follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from end2you.training import Losses\n",
    "\n",
    "\n",
    "class CustomMSELoss(Losses):\n",
    "    \n",
    "    def __init__(self, loss_name:str):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self._loss = self.custom_mse \n",
    "        self.loss_name = loss_name\n",
    "        \n",
    "    def custom_mse(self, \n",
    "                   predictions:torch.Tensor, \n",
    "                   labels:torch.Tensor):\n",
    "        \"\"\" Custom MSE loss function.\n",
    "        \n",
    "        Args:\n",
    "          predictions (torch.Tensor) (BS x 1): Model predictions\n",
    "          labels (torch.Tensor) (BS x 1): Data labels\n",
    "            BS: Batch size multiplied by the Sequence length.\n",
    "                e.g. batch_size = 10 and seq_length = 150\n",
    "                     => BS = 1500\n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = predictions.view(-1,)\n",
    "        labels = labels.view(-1,)\n",
    "        \n",
    "        return torch.mean((predictions - labels)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cls = CustomMSELoss('custom_MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the ability to define your own metric function and use it for training your model. To do so, you need to define a class that inherits from end2you's `MetricProvider` class and define your metric as method in the class. \n",
    "\n",
    "An example to define MSE metric follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from end2you.evaluation import MetricProvider\n",
    "\n",
    "\n",
    "class CustomMSEMetric(MetricProvider):\n",
    "    \n",
    "    def __init__(self, metric_name:str):\n",
    "        super(CustomMSEMetric, self).__init__()\n",
    "        self._metric = self.custom_mse\n",
    "        self.metric_name = metric_name\n",
    "    \n",
    "    def custom_mse(self, \n",
    "                   predictions:list, \n",
    "                   labels:list):\n",
    "        \"\"\" Custom MSE metric function.\n",
    "        \n",
    "        Args:\n",
    "          predictions (list): Model predictions of batch size length\n",
    "          labels (list): Data labels of batch size length\n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = np.concatenate(predictions).reshape(-1,)\n",
    "        labels = np.concatenate(labels).reshape(-1,)\n",
    "        \n",
    "        return np.mean((predictions - labels)**2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fn = CustomMSEMetric('custom_mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest of parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Params(dict_params={\n",
    "    'train':Params(dict_params={'cuda':True,  \n",
    "                                'optimizer':'adam',\n",
    "                                'learning_rate':0.0002,\n",
    "                                'summarywriter_file':'train_sw',\n",
    "                                'num_epochs':50,\n",
    "                                'batch_size':3,\n",
    "                                'save_summary_steps':10,\n",
    "                               }),\n",
    "    'valid':Params(dict_params={'cuda':True,  \n",
    "                                'modality':'audio',\n",
    "                                'batch_size':1, \n",
    "                              }),\n",
    "    'root_dir':'./path/to/save/output/files/of/end2you',\n",
    "    'log_file':'training.log',\n",
    "    'ckpt_path': None,\n",
    "    'num_gpus':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import end2you.training.optimizer as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "processes = ['train', 'valid']\n",
    "\n",
    "# Model\n",
    "num_model_params = [\n",
    "    pmt.numel() for pmt in model.parameters() if pmt.requires_grad is True]\n",
    "\n",
    "# Optimizer to choose\n",
    "optimizer = optim.get_optimizer(params.train.optimizer)\n",
    "optimizer = optimizer(model.parameters(), lr=params.train.learning_rate)\n",
    "\n",
    "tb_path = Path(params.root_dir) / 'summarywriters' \n",
    "summary_writers = {\n",
    "    process: SummaryWriter(str(tb_path / process))\n",
    "        for process in processes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = [str(x) for x in range(params.num_gpus)]\n",
    "device = torch.device(\"cuda:{}\".format(','.join(gpus)))\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info('Using', torch.cuda.device_count(), 'GPUs!')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from end2you.training import Trainer\n",
    "\n",
    "trainer = Trainer(loss=loss_cls, \n",
    "                  evaluator=eval_fn,\n",
    "                  data_providers=data_providers,\n",
    "                  summary_writers=summary_writers,\n",
    "                  root_dir=params.root_dir,\n",
    "                  model=model,\n",
    "                  ckpt_path=params.ckpt_path,\n",
    "                  optimizer=optimizer,\n",
    "                  params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
